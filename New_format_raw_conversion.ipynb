{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9d6830d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "from utility_automation import *\n",
    "from utility_Sparsity import *\n",
    "from utility_thresholding import *\n",
    "from utility_Feature_selaction import *\n",
    "import re\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54cd3fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = \"./config.json\"\n",
    "def load_config():\n",
    "    with open(config_path, \"r\") as f:\n",
    "        config = json.load(f)\n",
    "    return config\n",
    "config = load_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "507292d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function will config summary using given input config.\n",
    "\n",
    "def Config_Creation():\n",
    "    # load defination summary\n",
    "    df_summary=pd.read_csv(config[\"input_Data_summary\"])\n",
    "     # extracting the granurality level using def summary. simply it tell what level of granurality we are using for this brand (example:[Def L1, L1, Def L2, L2,.....Def Ln, Ln])\n",
    "    pattern = re.compile(r\"^L\\d(\\sDef)?$\")\n",
    "    granurality_level = [col for col in df_summary.columns if pattern.match(col)]\n",
    "    #Created a variable dictionary for all 6 variables with default values using config input.\n",
    "    variables={\"parent_level\":config[\"default_values\"][\"parent_level\"],\n",
    "                \"granularity_lavel_after_Parent_level\": len(granurality_level)/2-config[\"default_values\"][\"parent_level\"],\n",
    "                \"green_thr\": 5,\n",
    "                \"orange_thr\": 5,\n",
    "                \"green_active\": 10,\n",
    "                \"orange_active\": 10}\n",
    "    \n",
    "    \n",
    "    # Find Max value of parent_level whichever media type is using.\n",
    "    # initial it will take default value of parent_level and if any media type have greater than that it will get updated.\n",
    "    parent_level_max=int(variables['parent_level'])\n",
    "    for i in config['parent_level']:\n",
    "        if config['parent_level'][i]>parent_level_max:\n",
    "            parent_level_max=int(config['parent_level'][i])\n",
    "            \n",
    "    # Here we take columns only till parent_level_Max do make config summary   \n",
    "    #here we multiple with 2 beause to define one granurality we have to give 2 values (example : Def L1, L1)\n",
    "    #due to cutting till parent_level_max some rows have same values till granurality so now we will remore duplicate.\n",
    "    config_summary=df_summary[granurality_level[:parent_level_max*2]] \n",
    "    config_summary=config_summary.drop_duplicates()\n",
    "    config_summary=config_summary.reset_index().drop('index', axis=1)\n",
    "    \n",
    "    # here we assigne value for 6 variable fol all row of config_summary \n",
    "    # here it will check if any exception case (diffrent values from default value) avaibale then use that value otherwise use default values.\n",
    "    \n",
    "    for i in [\"parent_level\",\"granularity_lavel_after_Parent_level\", \"green_thr\", \"orange_thr\",\"green_active\",  \"orange_active\"]: # updating values for these Variable one by one\n",
    "        for index, row in config_summary.iterrows(): \n",
    "            f=False\n",
    "            for j in config[f'{i}']: #check for all available cases\n",
    "                L=j.split(\"|\") # make list of filter (assume Brand|Earn Media:3 for parent_level, then L will [\"Brand\", \"Earn Media\"])\n",
    "                # blew code will ensure that variables value only get overwrite from default value only for that which the satisfied gien conditon\n",
    "                is_subset = set(L).issubset(set(row.to_list()))\n",
    "                if is_subset:\n",
    "                    f=True\n",
    "                    break\n",
    "            if f:\n",
    "                config_summary.loc[index, f'{i}'] = config[f'{i}'][j] # assigning Defual value\n",
    "            else:\n",
    "                config_summary.loc[index, f'{i}']=variables[i]\n",
    "            #here we upadte the vlues for that who have diffrent value within it group\n",
    "            #(Example: for TV we take till CP level but within TV for TV Social we just want till CP then this functionalty will be useful )\n",
    "            if i!=\"parent_level\": # we are not updating the parent_level as it can't have diffrent value for subgroup\n",
    "                f=False\n",
    "                for j in config[f'{i}'][\"exception_case_in_Next_level\"]:\n",
    "                    L=j.split(\"|\")\n",
    "                    is_subset = set(L).issubset(set(row.to_list()))\n",
    "                    if is_subset:\n",
    "                        f=True\n",
    "                        break\n",
    "                if f:\n",
    "                    config_summary.loc[index, f'{i}'] = config[f'{i}'][\"exception_case_in_Next_level\"][j]\n",
    "                    \n",
    "    #above we took granurality till parent_level_max but different Media have different parent_level so we take according to individual parent_level\n",
    "    for index, row in config_summary.iterrows():\n",
    "        parent=int(row[\"parent_level\"])\n",
    "        config_summary.loc[index, config_summary.columns[parent*2:parent_level_max*2]] = np.nan\n",
    "    config_summary.drop_duplicates(inplace=True)\n",
    "    return config_summary, granurality_level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c7eb8ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function take as input Def Summary and AIO and give the Impressions and Cost according to date for all possible variables.\n",
    "\n",
    "def Format_Conversion(config):\n",
    "    #loading AIO file and Def_summary file\n",
    "    df_in=pd.read_csv(config[\"input_AIO_file_path\"])\n",
    "    df_in[config[\"date_col\"]] = pd.to_datetime(df_in[config[\"date_col\"]])\n",
    "    df_summary=pd.read_csv(config[\"input_Data_summary\"])\n",
    "    \n",
    "    #here we are defining one More metric with default value is \"Cost\". Metric 1 is user defined but Metric 2 is fixed and auto-creted here\n",
    "    df_summary[\"Metric 2\"]=\"Cost\"\n",
    "    \n",
    "    #with the help of pattern(\"Def L1\", \"L1\",.... ) in Def Summary columns, we extract the granularity_level.\n",
    "    # level_summary is created with thw help of granurality_lavel\n",
    "    pattern = re.compile(r\"^L\\d(\\sDef)?$\")\n",
    "    granurality_level = [col for col in df_summary.columns if pattern.match(col)]\n",
    "    level_summary = {granurality_level[i]: granurality_level[i+1] for i in range(0, len(granurality_level), 2)}\n",
    "    \n",
    "    #Filter out the data from AIO file accorung to Start date and End date.\n",
    "    filter_dict = {config[\"date_col\"] :list(pd.date_range(config[\"date_filter\"][\"start_date\"], config[\"date_filter\"][\"end_date\"]))}\n",
    "    for key in filter_dict.keys():\n",
    "        df_in = df_in[df_in[key].isin(filter_dict[key])]\n",
    "        \n",
    "    # generate filter and target dictionaries wrt summary sheet and raw data.\n",
    "    target_dict_Impression,target_dict_Cost, filter_dict = generate_filter_dict(\n",
    "                df_summary, config[\"common_filter\"], level_summary, config[\"target_metric\"]+[\"Metric 2\"]\n",
    "            )\n",
    "    # generate dict with filter tag and corresponding pivot dataframe.\n",
    "    tagged_df_Impresssion,tagged_df_cost = generate_tagged_df(\n",
    "                df_in,\n",
    "                config[\"common_filter\"],\n",
    "                filter_dict,\n",
    "                target_dict_Impression,\n",
    "                target_dict_Cost,\n",
    "                config['date_col'],\n",
    "                config[\"date_format\"],\n",
    "                config[\"prefix\"] ,)\n",
    "\n",
    "    df_merge_cost = generate_merge_df(tagged_df_cost, config['date_col'], config[\"date_filter\"][\"start_date\"], config[\"date_filter\"][\"end_date\"], config[\"date_format\"])\n",
    "    df_merge_cost = df_merge_cost.fillna(0)\n",
    "    df_merge_Impression = generate_merge_df(tagged_df_Impresssion, config['date_col'], config[\"date_filter\"][\"start_date\"], config[\"date_filter\"][\"end_date\"], config[\"date_format\"])\n",
    "    df_merge_Impression = df_merge_Impression.fillna(0)\n",
    "    \n",
    "    # df_merge_cost.to_csv(\"output/Cost.csv\", index=False)\n",
    "    # df_merge_Impression.to_csv('output/Impression.csv', index=False)\n",
    "    \n",
    "    return df_merge_Impression,  df_merge_cost\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4215c73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function create summary on daily level and weekly level for variables with some metrics (like %yeraly spend, %overall spend,  %yearly active, %overall active,...)\n",
    "\n",
    "def raw_conversion(imp, cost, config_summary, granurality_level):\n",
    "    imp_daily=pd.DataFrame()\n",
    "    imp_weekly=pd.DataFrame()\n",
    "    spend_daily=pd.DataFrame()\n",
    "    for i, row in config_summary.iterrows():\n",
    "        parent_level = int(row['parent_level'])\n",
    "        selected = row[:int(parent_level)*2]\n",
    "        value =  '|'.join(str(v) for v in selected if pd.notna(v) and str(v).strip() != \"\")\n",
    "        \n",
    "        # below code filter out those columns from input files which have same granurality till parent_level and \n",
    "        # from output the all metric will be calculated for each variables\n",
    "        col=[config['date_col']]+[col for col in imp.columns if value in col]\n",
    "        imp_t=imp[col]\n",
    "        cost_t=cost[col]\n",
    "        target_list=[\"Impressions/GRPs\", \"Spend\"]\n",
    "        for target in target_list:\n",
    "            if target==\"Impressions/GRPs\":\n",
    "                imp_daily_t=create_daily_data(imp_t, config['date_col'], target, granurality_level, parent_level) \n",
    "                imp_weekly_t=create_weekly_data(imp_t, config['date_col'], target, granurality_level, parent_level)\n",
    "            else:\n",
    "                spend_daily_t=create_daily_data(cost_t, config['date_col'], target, granurality_level, parent_level)           \n",
    "        imp_daily=pd.concat([imp_daily, imp_daily_t])\n",
    "        imp_weekly=pd.concat([imp_weekly, imp_weekly_t])\n",
    "        spend_daily=pd.concat([spend_daily, spend_daily_t])  \n",
    "           \n",
    "    # imp_daily.to_excel(\"output/imp_daily.xlsx\", index=False)\n",
    "    # imp_weekly.to_excel(\"output/imp_weekly.xlsx\", index=False)\n",
    "    # spend_daily.to_excel(\"output/spend_daily.xlsx\", index=False)\n",
    "    \n",
    "    #combining all metric together (from both weekly and daily)\n",
    "    pivot_df=create_pivot_ready_data(spend_daily,imp_daily,imp_weekly)\n",
    "    pivot_df.drop_duplicates(inplace=True)\n",
    "    \n",
    "    #here we are putting dummy values for empty cells so that we can use pivot table funtion in next step smoothly \n",
    "    #here dummy value can we anything but take care it shlod not be similare to operation or any media. we will remore this dummy values in next steps\n",
    "    for col in granurality_level:\n",
    "        pivot_df[col] = pivot_df[col].fillna(\"Raja_Babu\")\n",
    "        pivot_df[col] = pivot_df[col].astype('str').str.replace('0','None')\n",
    "    # pivot_df.to_excel(\"output/pivot_ready_data.xlsx\", index=False)\n",
    "    return pivot_df\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aa93229a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def thresholding_Conditions(filtered, merged_columns, granurality_level,granularity, summary_rows, green_thr, orange_thr, green_active, orange_active, len_granularity, parent_level):\n",
    "    value_list=[\n",
    "        \"%Weekly Overall Active\",\n",
    "        \"%Weekly Year Active\",\n",
    "        \"Overall Spend%\",\n",
    "        \"Year Spend%\",\n",
    "        \"Overall Spend\",\n",
    "        \"%Overall Spend Channel\",\n",
    "        \"%Year Spend Channel\"]\n",
    "\n",
    "    if len_granularity-2*parent_level-2*granularity==0:\n",
    "        pivot_index = granurality_level\n",
    "        pivot_values = value_list\n",
    "    else: \n",
    "        pivot_index = granurality_level[:-(len_granularity-2*parent_level-2*granularity)]\n",
    "        pivot_values = granurality_level[-(len_granularity-2*parent_level-2*granularity):] + value_list\n",
    "    table=create_pivot_table(filtered, pivot_index, pivot_values)\n",
    "    table_cpa=create_threshold_data(table,  green_thr, orange_thr, green_active, orange_active)\n",
    "    temp=create_summary(table, merged_columns, granularity, green_thr, orange_thr, green_active, orange_active)\n",
    "    summary_rows.append(temp)\n",
    "    granularity =1\n",
    "    pivot_index = granurality_level[:-(len_granularity-2*parent_level-2*granularity)]\n",
    "    pivot_values = granurality_level[-(len_granularity-2*parent_level-2*granularity):] + value_list\n",
    "    table=create_pivot_table(filtered, pivot_index, pivot_values)\n",
    "    table_cp=create_threshold_data(table, green_thr, orange_thr, green_active, orange_active)\n",
    "    temp=create_summary(table, merged_columns, granularity, green_thr, orange_thr, green_active, orange_active)\n",
    "    summary_rows.append(temp)\n",
    "    return table_cpa, table_cp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "044412ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Variable_list_feature_list_generation(pivot_df, granurality_level, config_summary):    \n",
    "    Variable_List=pd.DataFrame()\n",
    "    feature_list=pd.DataFrame()\n",
    "    summary_rows = []\n",
    "    for i, row in config_summary.iterrows():\n",
    "        parent_level = int(row['parent_level'])\n",
    "        selected = row[:parent_level*2].dropna().tolist()\n",
    "        cols_to_match = config_summary.columns[:len(selected)]\n",
    "        mask = (pivot_df[cols_to_match] == row[cols_to_match]).all(axis=1)\n",
    "        merged_columns =  '|'.join(str(v) for v in selected if pd.notna(v) and str(v).strip() != \"\")\n",
    "        filtered = pivot_df[mask].copy()\n",
    "        if filtered.shape[0] == 0:\n",
    "            print(\"No data\")\n",
    "            continue\n",
    "        green_thr=row[\"green_thr\"]\n",
    "        orange_thr=row[\"orange_thr\"]\n",
    "        green_active= row[\"green_active\"]\n",
    "        orange_active=row[\"orange_active\"]\n",
    "        granularity_lavel_after_Parent_level=int(row[\"granularity_lavel_after_Parent_level\"])\n",
    "        len_granularity=int(len(granurality_level))\n",
    "        sheet_cpa, sheet_cp=thresholding_Conditions(filtered, merged_columns, granurality_level, granularity_lavel_after_Parent_level, summary_rows, green_thr, orange_thr, green_active, orange_active, len_granularity, parent_level)\n",
    "        sheet_cpa['Colour'] = sheet_cpa[['Overall Green','Overall Orange']].apply(color,axis=1) \n",
    "        sheet_cpa=Merging_Type(sheet_cpa,sheet_cp, parent_level, orange_thr)\n",
    "        Variable_List= pd.concat([Variable_List, sheet_cpa], ignore_index=True)\n",
    "        #implemently audience maping and platform mapping only for paid media\n",
    "        if \"|Paid Media|\" in merged_columns and \"|Halo|\" not in merged_columns and \"|Master Brand|\" not in merged_columns:\n",
    "            sheet_cpa=Audience_Others(sheet_cpa, parent_level, granularity_lavel_after_Parent_level, len_granularity)\n",
    "            sheet_cpa=Platform_Others(sheet_cpa, parent_level,granularity_lavel_after_Parent_level,len_granularity)\n",
    "        feature_list=pd.concat([feature_list, sheet_cpa], ignore_index=True)\n",
    "    \n",
    "    #here we will remore that dummy values from variable list and feature list   \n",
    "    pattern = re.compile(r\"^Raja_Babu$\", re.IGNORECASE)\n",
    "    for col in Variable_List.select_dtypes(include=['object']).columns:\n",
    "        Variable_List[col] = Variable_List[col].apply(lambda x: '' if isinstance(x, str) and pattern.match(x) else x)\n",
    "    for col in feature_list.select_dtypes(include=['object']).columns:\n",
    "        feature_list[col] = feature_list[col].apply(lambda x: '' if isinstance(x, str) and pattern.match(x) else x)\n",
    "    feature_list=feature_list[granurality_level+['Merging type']]\n",
    "    feature_list[\"Remarks\"] = None\n",
    "    return Variable_List, feature_list, feature_list.drop_duplicates() , pd.DataFrame(summary_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e9039b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_summary,granurality_level =Config_Creation()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9be74388",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ps/y9g7f0_52yggnl9pvx1l76zh0000gp/T/ipykernel_12269/2346988458.py:5: DtypeWarning: Columns (14,23,28,30,31) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_in=pd.read_csv(config[\"input_AIO_file_path\"])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged df using the dictionary. The final shape is (1581, 331)\n",
      "Merged df using the dictionary. The final shape is (1581, 331)\n"
     ]
    }
   ],
   "source": [
    "imp, cost=Format_Conversion(config)\n",
    "pivot_df=raw_conversion(imp, cost, config_summary, granurality_level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5006fd60",
   "metadata": {},
   "outputs": [],
   "source": [
    "Variable_List, feature_list_with_duplicates, feature_list,summary=Variable_list_feature_list_generation(pivot_df, granurality_level, config_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9e8d6e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create copies to avoid changing the original DataFrames\n",
    "feature_list_copy = feature_list_with_duplicates.copy()\n",
    "variable_list_copy = Variable_List.copy()\n",
    "\n",
    "# Add MultiIndex column headers to the copies\n",
    "feature_list_copy.columns = pd.MultiIndex.from_product([[\"Default Feature List\"], feature_list_copy.columns])\n",
    "variable_list_copy.columns = pd.MultiIndex.from_product([[\"Default Variable List\"], variable_list_copy.columns])\n",
    "\n",
    "# Concatenate side by side (new DataFrame)\n",
    "final_df = pd.concat([feature_list_copy, variable_list_copy], axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "86d95761",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Export all to one Excel file with multiple sheets\n",
    "with pd.ExcelWriter('output/Feature_List.xlsx', engine='openpyxl') as writer:\n",
    "    summary.to_excel(writer, sheet_name='Summary', index=False)\n",
    "    config_summary.to_excel(writer, sheet_name='Config_Summary', index=False)\n",
    "    Variable_List[granurality_level+[\"Merging type\"]].to_excel(writer, sheet_name='Variable_List', index=False)\n",
    "    feature_list.to_excel(writer, sheet_name='Feature_List', index=False)\n",
    "    final_df.to_excel(writer, sheet_name='Tracker_Dict', index=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d3e45f4",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
